# -*- coding: utf-8 -*-
"""Image Captioning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WDzE9E-cuT7mIkq8jDnrK6kviqTzk7vc
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import keras
import re
import nltk
from nltk.corpus import stopwords
import json
import pickle
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import *
from tensorflow.keras.layers.merge import add
from tqdm import tqdm
import os

df = pd.read_csv('./Desktop/TechVidvan/Image Caption/captions.txt')

df.head()

df.tail()

X = df.values
X[-1]

def clean_text(s) :
    """" To clean the Text """
    s = s.lower()
    s = re.sub('[^a-z]+', " ", s)
    s = s.split()
    s = ' '.join(s)
    return s

all_img_captions = {}

for i in tqdm(X):
    if all_img_captions.get(i[0]) is None:
        all_img_captions[i[0]] = []

    all_img_captions[i[0]].append("<start> " + clean_text(i[1]) + " <end>")

all_img_captions['997722733_0cb5439472.jpg']

img = image.load_img('./Desktop/TechVidvan/Image Caption/Images/997722733_0cb5439472.jpg')
plt.figure(figsize= (10,10))
plt.imshow(img)
plt.axis('off')
plt.show()

# creating a vocab
vocabulary = set()
all_words = []

for values in tqdm(all_img_cap.values()):
    for line in values:
        [vocabulary.add(word) for word in line.split()]
        [all_words.append(word) for word in line.split()]

len(vocabulary), len(all_words)

import collections
counter = collections.Counter(all_words)
freq_cnt = dict(counter)
freq_cnt = sorted(freq_cnt.items(), reverse= True, key= lambda x:x[1])

# Filter
threshold = 10
freq_cnt = [x for x in freq_cnt if x[1] >= threshold]
total_words = [x[0] for x in freq_cnt]

len(freq_cnt), len(total_words)

word_2_idx = {}
idx_2_word = {}
i = 1

for word in total_words:
    word_2_idx[word] = i
    idx_2_word[i] = word
    i += 1

max_len = 0
for key in all_img_cap.keys():
    for line in all_img_cap[key]:
        max_len = max(max_len, len(line.split()))

print(max_len)

vocab_size = len(word_2_idx) + 1
vocab_size

f = open('./Desktop/TechVidvan/Image Caption/glove.6B.50d.txt', encoding= 'utf8')

emb_index = {}

for line in tqdm(f):
    values = line.split()
    word = values[0]
    emb = np.array(values[1:], dtype= 'float')
    emb_index[word] = emb

f.close()

def get_emb_matrix():
    emb_dim = 50
    matrix = np.zeros((vocab_size, emb_dim))

    for word, idx in word_2_idx.items():
        emb_vec = emb_index.get(word)

        if emb_vec is not None:
            matrix[idx] = emb_vec
        
    return matrix

emb_matrix = get_emb_matrix()

emb_matrix.shape

model = ResNet50(weights = 'imagenet', input_shape = (224, 224, 3))

new_model = Model(inputs = model.input, outputs = model.layers[-2].output)
new_model.summary()
# model.save('/content/Gdrive/My Drive/img cap/img_pre_model.h5')

def proc_img(img_path) :
    img = image.load_img(img_path, target_size= (224, 224))
    img = image.img_to_array(img)
    img = img.reshape((1, 224, 224, 3))
    img = preprocess_input(img)
    img = new_model.predict(img)
    
    return img.reshape((-1,))

img_encoded = {}
base_path = './Desktop/TechVidvan/Image Caption/Images/'
for img in tqdm(os.listdir(base_path)):
    img_encoded[img] = proc_img(base_path+img)

img_encoded['1827560917_c8d3c5627f.jpg'].shape

def train_data_generator(all_img_cap, img_encoded, word_2_idx, max_len, vocab_size, batch_size) :
    X1, X2, y1 = [], [], []
    n = 0

    while True :
        for key in all_img_cap.keys():
            n += 1

            photo = img_encoded[key]

            for cap in all_img_cap[key] : # for each caption of that img
                seq = [word_2_idx[word] for word in cap.split() if word in word_2_idx]

                for i in range(1, len(seq)) :
                    xi = seq[0:i]
                    yi = seq[i]

                    # padding xi
                    xi = pad_sequences([xi], maxlen= max_len, padding= 'post', value= 0)[0]
                    # converting y to onehot values
                    yi = to_categorical([yi], num_classes= vocab_size)[0]

                    X1.append(photo)
                    X2.append(xi)
                    y1.append(yi)
                
            if n == batch_size:
                yield [[np.array(X1), np.array(X2)], np.array(y1)]
                X1, X2, y1 = [], [], []
                n = 0

# Image Input
inp_img_feature = Input(shape = (2048, ))
inp_img1 = Dropout(rate = 0.3) (inp_img_feature)
inp_img1 = Dense(256, activation= 'relu') (inp_img1)

# Caption Input
inp_cap = Input(shape= (max_len,))
inp_cap1 = Embedding(input_dim= vocab_size, output_dim= 50, mask_zero= True) (inp_cap)
inp_cap1 = LSTM(256) (inp_cap1)

decoder1 = add([inp_img1, inp_cap1])
decoder2 = Dense(256, activation= 'relu') (decoder1)
output = Dense(vocab_size, activation= 'softmax') (decoder2)

model = Model(inputs= [inp_img_feature, inp_cap], outputs = [output])

model.summary()

model.layers[3].set_weights([emb_matrix])
model.layers[3].trainable = False

model.summary()

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics= ['accuracy'])

epoch = 50
batch_size = 512
step = 32

loss = []
acc = []

def train(batch_size, epoch, step):
    for i in range(epoch):
        generator = train_data_generator(all_img_cap, img_encoded,word_2_idx, max_len, vocab_size, batch_size)
        model.fit_generator(generator, steps_per_epoch= step, verbose= 1)
        loss.append(model.history.history['loss'])
        acc.append(model.history.history['accuracy'])
        # model.save('./Desktop/TechVidvan/Image Caption/img cap/model'+str(i+100)+'.h5')

train(batch_size, epoch, step)

plt.plot(acc)
plt.show()

plt.plot(loss)
plt.show()

img_pre_model = load_model('./Desktop/TechVidvan/Image Caption/img cap/img_pre_model.h5')
img_pre_model = Model(inputs = img_pre_model.input, outputs = img_pre_model.layers[-2].output)

model = load_model('./Desktop/TechVidvan/Image Caption/img cap/model149.h5')
model.summary()

max_len = 38

def preprocess_img(photo_path):
    img = image.load_img(photo_path, target_size= (224, 224))
    plt.imshow(img)
    plt.axis("off")
    plt.show()
    img = image.img_to_array(img)
    img = img.reshape((1, 224, 224, 3))
    img = preprocess_input(img)
    img = img_pre_model.predict(img)
    
    return img

def predict_caption(photo_path,max_len, word_2_idx, idx_2_word):
    img = preprocess_img(photo_path)
    text = "startseq"

    for i in range(max_len) :
        seq = [word_2_idx[word] for word in text.split() if word in word_2_idx]
        seq = pad_sequences([seq], maxlen= max_len, padding= 'post')
        y_pred = model.predict([img, seq])
        y_pred = y_pred.argmax()

        word = idx_2_word[str(y_pred)]
        text += " " + word

        if word == "endseq":
            break
    
    cap = text.split()[1:-1]
    cap = " ".join(cap)

    return cap

photo_path = './Desktop/TechVidvan/Image Caption/image.jpg'
predict_caption(photo_path, max_len, word_2_idx, idx_2_word)

